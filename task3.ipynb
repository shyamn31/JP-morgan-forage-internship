{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df29eda",
   "metadata": {
    "id": "1df29eda"
   },
   "source": [
    "Step 0. Unzip enron1.zip into the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32cfce",
   "metadata": {
    "id": "bf32cfce"
   },
   "source": [
    "Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you and should run without any errors. You should recognize Pandas from task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c5d195",
   "metadata": {
    "id": "20c5d195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 2248.2004-09-23.GP.spam.txt\n",
      "skipped 2526.2004-10-17.GP.spam.txt\n",
      "skipped 2698.2004-10-31.GP.spam.txt\n",
      "skipped 4566.2005-05-24.GP.spam.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_spam():\n",
    "    category = 'spam'\n",
    "    directory = './enron1/spam'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_ham():\n",
    "    category = 'ham'\n",
    "    directory = './enron1/ham'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_category(category, directory):\n",
    "    emails = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), 'r') as fp:\n",
    "            try:\n",
    "                content = fp.read()\n",
    "                emails.append({'name': filename, 'content': content, 'category': category})\n",
    "            except:\n",
    "                print(f'skipped {filename}')\n",
    "    return emails\n",
    "\n",
    "ham = read_ham()\n",
    "spam = read_spam()\n",
    "\n",
    "df = pd.DataFrame.from_records(ham)\n",
    "df = pd.concat([df, pd.DataFrame.from_records(spam)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c23fd",
   "metadata": {
    "id": "1a1c23fd"
   },
   "source": [
    "Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c447c901",
   "metadata": {
    "id": "c447c901"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(e):\n",
    "    return re.sub('[^A-Za-z]', ' ', e).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32521d",
   "metadata": {
    "id": "ba32521d"
   },
   "source": [
    "Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1442d377",
   "metadata": {
    "id": "1442d377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9748549323017408\n",
      "\n",
      "Confusion Matrix:\n",
      "[[729  16]\n",
      " [ 10 279]]\n",
      "\n",
      "Detailed Statistics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.98       745\n",
      "        spam       0.95      0.97      0.96       289\n",
      "\n",
      "    accuracy                           0.97      1034\n",
      "   macro avg       0.97      0.97      0.97      1034\n",
      "weighted avg       0.98      0.97      0.97      1034\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shyam\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessor)\n",
    "X_train,X_test,y_train,y_test = train_test_split(df[\"content\"],df[\"category\"],test_size=0.2,random_state=1)\n",
    "\n",
    "\n",
    "X_train_df = vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_df,y_train)\n",
    "\n",
    "\n",
    "X_test_df = vectorizer.transform(X_test)\n",
    "y_pred = model.predict(X_test_df)\n",
    "\n",
    "\n",
    "print(f'Accuracy:\\n{accuracy_score(y_test,y_pred)}\\n')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,y_pred)}\\n')\n",
    "print(f'Detailed Statistics:\\n{classification_report(y_test,y_pred)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674d032",
   "metadata": {
    "id": "9674d032"
   },
   "source": [
    "Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b7d78c9",
   "metadata": {
    "id": "6b7d78c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.9465430515498962 http\n",
      "0.8662010695956848 prices\n",
      "0.8151424869384507 no\n",
      "0.7513520177815852 pain\n",
      "0.722508455690836 only\n",
      "0.7190209612640248 paliourg\n",
      "0.7186432269269624 money\n",
      "0.693039324056552 more\n",
      "0.6866479314882128 remove\n",
      "0.6802226547274995 removed\n",
      "\n",
      "-1.5992384489428075 enron\n",
      "-1.5480715450490574 attached\n",
      "-1.4053750528481797 daren\n",
      "-1.382260089161263 thanks\n",
      "-1.3564449591282988 doc\n",
      "-1.185896700897139 deal\n",
      "-1.1421505214315102 xls\n",
      "-1.124158355843412 meter\n",
      "-1.0600763079219155 hpl\n",
      "-1.0216031374860164 neon\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "importance = model.coef_[0]\n",
    "l = list(enumerate(importance))\n",
    "print()\n",
    "l.sort(key=lambda e: e[1], reverse=True)\n",
    "for i,imp in l[:10]:\n",
    "    print(imp, features[i])\n",
    "print()\n",
    "l.sort(key=lambda e: -e[1], reverse=True)\n",
    "for i,imp in l[:10]:\n",
    "    print(imp, features[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267e7ad",
   "metadata": {
    "id": "d267e7ad"
   },
   "source": [
    "Submission\n",
    "1. Upload the jupyter notebook to Forage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LI4u_ZUGToDQ",
   "metadata": {
    "id": "LI4u_ZUGToDQ"
   },
   "source": [
    "All Done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
